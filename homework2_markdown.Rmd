---
title: "Homework 2"
output:
  word_document: default
  html_notebook: default
  pdf_document: default
  html_document:
    theme: readable
---



```{r}
options(scipen=999)

```





#Problem 1(a) - NB by hand 



```{r, message=FALSE, warning=FALSE}
#Estimate the liklihood that the following e-mail was sent from Marine Le Pen or Emmanuel Macron. 

email <- c("immigration", "voter", "culture", "help", "neighborhood")

lepen1 <- c("immigration", "women", "assimilate", "afraid", "win")
lepen2 <- c("culture", "voter", "economy", "president", "help")
macron1 <- c("voter", "women", "help", "reform", "education")
macron2 <- c("union", "economy", "hope", "immigration", "neighborhood")
macron3 <- c("win", "union", "europe", "elect", "president")
lepen3 <- c("economy", "voter", "immigration", "president", "culture")
macron4 <- c("success", "german", "culture", "help", "french")

#calculate estimated probability of a given class from training data by dividing number of ocurrences of class by
#total number of documents
pr_c_lepen <- 3/7
pr_c_macron <- 4/7

#calculate estimated probability of a term given a class from training data by dividing number of term occurrences in 
#respective class (including multiple occurrences by total number of all terms in the training documents 


pr_immigration_lepen <- 2/15
pr_voter_lepen <- 2/15
pr_culture_lepen <- 2/15
pr_help_lepen <- 1/15
pr_neighborhood_lepen <- 0/15

pr_email_lepen <- (2/15) * (2/15) * (2/15) * (1/15) * (0/15) * (3/7)

pr_immigration_macron <- 1/20
pr_voter_macron <- 1/20
pr_culture_macron <- 1/20
pr_help_macron <- 2/20
pr_neighborhood_macron <- 1/20

pr_email_macron <- (1/20) * (1/20) * (1/20) * (2/20) * (1/20) * (4/7)

```

#Problem 1(a) - answer - NB by hand 

#####I don't completely trust these findings. Because LePen never uses the word "neighborhood" in the training documents, the value for the maximum likelihood estimation for this term is 0. This sparsity cancels out the other MLEs, making the class probability estimate 0, even if the MLEs for the other tems are high. 


```{r, message=FALSE, warning=FALSE}
pr_email_lepen

pr_email_macron

```






#Problem 1(b) - NB by hand with LaPlace smoothing 


```{r, message=FALSE, warning=FALSE}

#Perform LaPlace smoothing by adding 1 to each value 

#class probability does not change
pr_c_lepen <- 3/7
pr_c_macron <- 4/7

#add one to each value, including total term count 


pr_immigration_lepen <- 3/30
pr_voter_lepen <- 3/30
pr_culture_lepen <- 3/30
pr_help_lepen <- 2/30
pr_neighborhood_lepen <- 1/30

pr_email_lepen <- (3/30) * (3/30) * (3/30) * (2/30) * (1/30) * (3/7)


pr_immigration_macron <- 2/40
pr_voter_macron <- 2/40
pr_culture_macron <- 2/40
pr_help_macron <- 3/40
pr_neighborhood_macron <- 2/40

pr_email_macron <- (2/40) * (2/40) * (2/40) * (3/40) * (2/40) * (4/7)

```


#Problem 1(b) - answer - NB by hand with LaPlace smoothing 
#####Because we added 1 to each term, thus blunting the effect of the sparse terms, LePen is now the more likely candidate to have written the e-mail. 


```{r, message=FALSE, warning=FALSE}
pr_email_lepen

pr_email_macron 

```



#Problem 2(a) - creating positive and negative classifier 

#####Preprocessing: Because we are simply creating a new variable, there is not much preprocessing of the text neccesary for this particular step. 



```{r, message=FALSE, warning=FALSE}
amzn <- read.csv("amazon_reviews.csv", stringsAsFactors = FALSE)
names(amzn)[1] <- "ID"
smed_score <- median(amzn$Score)

amzn$Class <- ifelse(amzn$Score <= 3, 0, 1)


```


#Problem 2(a) - answer - creating positive and negative classifier 


#####Preprocessing: As we are actually dealing with the text, I will perform various preprocessing tasks. First, I will remove the encoding errors, as they will show up as words when tokenized. Then, I'll remove stopwords and punctuation, in addition to converting to lower case and stemming to account for for different tenses and spelling errors. 

```{r, message=FALSE, warning=FALSE}

#Actual Score 
summary(amzn$Class)
```

#Problem 2(a) - creating anchor variables 


```{r, message=FALSE, warning=FALSE}



amzn$Anchor <- ifelse(amzn$Score == 5 | amzn$Score == 1, 1, 0)
amzn$Anchor_Neg <- ifelse(amzn$Score ==1, -1, 0)

for (i in 1:nrow(amzn)) {
  if (amzn$Score[i] == 5) {
    amzn$Anchor_Combo[i] <- 1
  }
  else if (amzn$Score[i] == 1)
  {
    amzn$Anchor_Combo[i] <- -1
  }
  
  else {
    amzn$Anchor_Combo[i] <- 0
  }
}



```


#Problem 2(a) - answer - creating anchor variables
```{r, message=FALSE, warning=FALSE}

#anchor

summary(amzn$Anchor)

#anchor negative 
summary(amzn$Anchor_Neg)


```

#Problem 2(b) - training NB to predict positive/negative reviews 


```{r, message=FALSE, warning=FALSE}

#Read in sentiment dictionary by Hu & Liu 
library(quanteda)
#read files and convert to vectors
neg_sent <- read.csv("negative-words.txt", sep=" ", header=FALSE, stringsAsFactors = FALSE)
pos_sent <- read.csv("positive-words.txt", sep=" ", header=FALSE, stringsAsFactors = FALSE)
pos_sent <- as.vector(pos_sent$V1)
neg_sent <- as.vector(neg_sent$V1)
#create sentiment dictionary with positive and negative values 
sent_dict <- dictionary(positive=pos_sent, negative=neg_sent)

#remove escape characters and unicode
amzn$Text <- gsub("[&#][1-9]*", "", amzn$Text, fixed=TRUE)
#create corpus with appropriate variables
amzn.corp <- corpus(amzn$Text, docnames=amzn$ID, docvars=data.frame(class=amzn$Class, anchor_combo = amzn$Anchor_Combo, ID=amzn$ID))

#create dfm with dictionary 
amzn.dfm <- dfm(tokenize(amzn.corp, removePunct=TRUE), dictionary=sent_dict, stem=TRUE, tolower=TRUE)

#convert to dfm and add postive and negative counts to df
amzn.df <- as.data.frame(amzn.dfm)
amzn <- cbind(amzn, amzn.df)

#calculate sentiment score
amzn$Sent_Score <- amzn$posit - amzn$negat

#create class for new sentiment score 
amzn$Sent_Class <- ifelse(amzn$Sent_Score >= 0, 1, 0)


```


#Problem 2(b) - answer - sentiment score 


#####Pre-processing: The preprocessing for this step was similar to the previous question. I removed all non-alphanumeric characters and punctuation, while stemming words and converting them to lowercase. Again, I performed this step so to account for variations in tense and spelling. 

```{r, message=FALSE, warning=FALSE}

#sentiment score 
summary(amzn$Sent_Score) 


```

#Problem 2(b) - answer - sentiment class 
```{r, message=FALSE, warning=FALSE}

#sentiment class 
summary(amzn$Sent_Class) 


```

```{r, message=FALSE, warning=FALSE}


#create histogram vizualizing distribution of positive versus negative words
score_min <- min(amzn$Sent_Score)
score_max <- max(amzn$Sent_Score)

```


#Problem 2(b) - answer - histogram 
```{r, message=FALSE, warning=FALSE}

hist(amzn$Sent_Score, breaks="Sturges", freq=TRUE, xlab="Sentiment Score", main="Distribution of Sentiment Scores", plot=TRUE, xlim=c(score_min, score_max))

```





#Problem 2(b) - creating confusion matrix and accuracy scores 




```{r, message=FALSE, warning=FALSE}
#calculate percent positive 
pct_pos <- (nrow(amzn[amzn$Sent_Class == 1 ,]) / nrow(amzn)) *100
paste(pct_pos, "% positive", sep="")

#create confusion matrix
confu_mat <- table(amzn$Sent_Class, amzn$Class)
colnames(confu_mat) <- c("New_Score: Negative", "New_Score: Positive")
rownames(confu_mat) <- c("Original_Score: Negative", "Original_Score: Positive")

#calculate accuracy 
acc <- sum(confu_mat[c(1, 4)])/sum(confu_mat) * 100

#calculate precision 
prec <- round(sum(confu_mat[4])/sum(confu_mat[c(4, 3)]) * 100, 2)

#calculate recall 
recall <- round(sum(confu_mat[4])/sum(confu_mat[c(4, 2)]) * 100, 2)


```

#Problem 2(b) - answer - creating confusion matrix and accuracy scores 


```{r, message=FALSE, warning=FALSE}

confu_mat
paste("Accuracy:", acc, "%", sep="")
paste("Precision:", prec, "%", sep="")
paste("Recall:", recall, "%", sep="")


```


#Problem 2(b) - generate rank for scores and compute absolute difference 




```{r, message=FALSE, warning=FALSE}

#rank by new score so that lower values come first
amzn <- amzn[with(amzn, order(amzn$Sent_Score, decreasing=TRUE)) ,]
#add rank from 1 to 10,000
amzn$Sent_Rank <- seq(1:10000)

#rank by original score so that lower values come first
amzn <- amzn[with(amzn, order(amzn$Score, decreasing=TRUE)) ,]
#add rank from 1 to 10,000
amzn$Act_Rank <- seq(1:10000)
#compute abs difference between two ranks
rank_sum_dict <- sum(abs(amzn$Sent_Rank - amzn$Act_Rank))



```


#Problem 2(b) - answer - generate rank for scores and compute absolute difference 


```{r, message=FALSE, warning=FALSE}

rank_sum_dict

``` 



#Problem 2(c) - creating NB classifier 

#####Pre-processing: Preprocessing in this question was similar to the previous ones.


```{r, message=FALSE, warning=FALSE}

amzn.corp <- corpus(amzn$Text, docvars=data.frame(ID= amzn$ID, class=amzn$Class, score = amzn$Score, anchor=amzn$Anchor, anchor_neg = amzn$Anchor_Neg))
#create training and test sets
samp <- floor(0.20 * nrow(amzn))
set.seed(100)
train_set <- sample(seq_len(nrow(amzn)), size=samp)
train.df <- amzn[train_set,]
test.df <- amzn[-train_set,]
amzn$Split <- ifelse(amzn$ID %in% train.df$ID,  "train", "test")

#create test and training corpus 
corp.train <- corpus_subset(amzn.corp, subset= amzn.corp$documents$ID %in% train.df$ID, select=c(class, ID))
corp.test <- corpus_subset(amzn.corp, subset= amzn.corp$documents$ID %in% test.df$ID, select=c(class, ID))

#create dfm with training set and save class labels
train.dfm <- dfm(tokenize(corp.train, removePunct=TRUE),dictionary=sent_dict, groups=corp.train$documents$ID, stem=TRUE, tolower=TRUE)
labelids <- as.data.frame(train.dfm@Dimnames$docs)
names(labelids) <- "ID"
library(plyr)
labeldf <- join(labelids, train.df)
labels <- labeldf$Class


# train NB model with "uniforms" as prior 
nb.dict <- textmodel_NB(x=train.dfm, y=labels, data=NULL,
                   smooth=1, prior="uniform") 

#create dfm for test to calculate accuracy, precision and recall 
test.dfm <- dfm(tokenize(corp.test, removePunct=TRUE),dictionary=sent_dict, groups=corp.test$documents$ID, stem=TRUE, tolower=TRUE)
test.pr1 <- predict(nb.dict, newdata=test.dfm)

#convert to df and add original class labels 
pred.df <- as.data.frame(test.pr1[1:4])
pred.df$ID <-as.integer(rownames(pred.df))
pred.df <- merge(x=pred.df, y=test.df, by="ID")

#create confusion matrix and add labels
library(caret)
confu_mat2 <- confusionMatrix(pred.df$nb.predicted, pred.df$Class)
confu_mat2 <- confu_mat2$table


#calculate accuracy 
acc <- sum(confu_mat2[c(1, 4)])/sum(confu_mat2) * 100

#calculate precision 
prec <- round(sum(confu_mat2[4])/sum(confu_mat2[c(4, 3)]) * 100, 2)

#calculate recall 
recall <- round(sum(confu_mat2[4])/sum(confu_mat2[c(4, 2)]) * 100, 2)

```

#Problem 2(c) - answer - NB uniform priors 

```{r, message=FALSE, warning=FALSE}

confu_mat2
paste("Accuracy:", acc, "%", sep="")
paste("Precision:", prec, "%", sep="")
paste("Recall:", recall, "%", sep="")


```

#Problem 2(c) - NB with "docfreq""






```{r, message=FALSE, warning=FALSE}

amzn.corp <- corpus(amzn$Text, docvars=data.frame(ID= amzn$ID, class=amzn$Class, score = amzn$Score, anchor=amzn$Anchor, anchor_neg = amzn$Anchor_Neg))
#create training and test sets
samp <- floor(0.20 * nrow(amzn))
set.seed(100)
train_set <- sample(seq_len(nrow(amzn)), size=samp)
train.df <- amzn[train_set,]
test.df <- amzn[-train_set,]
amzn$Split <- ifelse(amzn$ID %in% train.df$ID,  "train", "test")

#create test and training corpus 
corp.train <- corpus_subset(amzn.corp, subset= amzn.corp$documents$ID %in% train.df$ID, select=c(class, ID))
corp.test <- corpus_subset(amzn.corp, subset= amzn.corp$documents$ID %in% test.df$ID, select=c(class, ID))

#create dfm with training set and save class labels
train.dfm <- dfm(tokenize(corp.train, removePunct=TRUE),dictionary=sent_dict, groups=corp.train$documents$ID, stem=TRUE, tolower=TRUE)
labelids <- as.data.frame(train.dfm@Dimnames$docs)
names(labelids) <- "ID"
library(plyr)
labeldf <- join(labelids, train.df)
labels <- labeldf$Class




# train NB model with "docfreq" as prior 
nb.dict <- textmodel_NB(x=train.dfm, y=labels, data=NULL,
                   smooth=1, prior="docfreq") 

#create dfm for test to calculate accuracy, precision and recall 
test.dfm <- dfm(tokenize(corp.test, removePunct=TRUE),dictionary=sent_dict, groups=corp.test$documents$ID, stem=TRUE, tolower=TRUE)
test.pr1 <- predict(nb.dict, newdata=test.dfm)

#convert to df and add original class labels 
pred.df <- as.data.frame(test.pr1[1:4])
pred.df$ID <-as.integer(rownames(pred.df))
pred.df <- merge(x=pred.df, y=test.df, by="ID")

#create confusion matrix and add labels
library(caret)
confu_mat2 <- confusionMatrix(pred.df$nb.predicted, pred.df$Class)
confu_mat2 <- confu_mat2$table

confu_mat2

#calculate accuracy 
acc <- sum(confu_mat2[c(1, 4)])/sum(confu_mat2) * 100

#calculate precision 
prec <- round(sum(confu_mat2[4])/sum(confu_mat2[c(4, 3)]) * 100, 2)

#calculate recall 
recall <- round(sum(confu_mat2[4])/sum(confu_mat2[c(4, 2)]) * 100, 2)

```




#Problem 2(c) - answer - NB witn "docfreq"

#####I would expect changing the priors from uniform to "docfreq" to decrease the accuracy of the model. Because using document frequency would imply the Bernoulli model, which ignores multiple ocurrences of a term in a document, I would expect that this would weaken the predicative power of the model, as it is one piece less of information. As long as we are confident in our class labels, looking at term ocurrence as a fraction of all words in a particular class (i.e. uniform) should provide enough information to take into account the varying frequency of terms across documents of the same class. 


```{r, message=FALSE, warning=FALSE}

confu_mat2

paste("Accuracy:", acc, "%", sep="")
paste("Precision:", prec, "%", sep="")
paste("Recall:", recall, "%", sep="")



```

#Problem 2(c) - answer - removing smoothing from NB


#####Because the test set may contain terms that were not used in the training set, those documents will be given a zero probability unless you add some sort of smoothing to account for this sparsity. 


#Problem 2(d) - wordscores model 
#####Pre-processing: Preprocessing in this question was similar to the previous ones, however I did add smoothing to the DFM in order to score it against the wordscores model with smoothing. 


```{r, message=FALSE, warning=FALSE}


#subset original df to just include extreme anchors
amzn_anch <- amzn[amzn$Anchor_Combo %in% c(1, -1) ,]

#create corpus 
anch.corp <- corpus(amzn_anch$Text, docnames=amzn_anch$ID, docvars=data.frame(class=amzn_anch$Class, anchor_combo = amzn_anch$Anchor_Combo, ID=amzn_anch$ID))

#create dfm 
pos_neg_dfm <- dfm(tokenize(anch.corp, removePunct=TRUE), stem=TRUE, tolower=TRUE, remove=stopwords("english"))

#create vector with labels 
labels <- amzn_anch$Anchor_Combo

#run wordscore model with smoothing factor of 1
ws_smooth<-textmodel(pos_neg_dfm, 
                     y = labels, 
                     model="wordscores", smooth=1)


#create vector of terms and their respective weights
feats<- sort(ws_smooth@Sw, decreasing=TRUE)


```


#Problem 2(d) - answer - wordscores model 
#####On the positive end of the spectrum, some of the most powerful words are "season", "love", "great" and "enjoy", while on the negative end, some of the more common words are "terrible", "get", "seem", "stupid". 

```{r, message=FALSE, warning=FALSE}

#positive features
head(feats, 10) 

#negative features
tail(feats, 10)


```

#Problem 2(d) - create rank sum variable for wordscores 


```{r, message=FALSE, warning=FALSE}

amzn.dfm_smooth <- dfm_smooth(amzn.dfm, smoothing=1)
#create wordscore prediction model 
ws_prediction <- predict(ws_smooth, newdata = amzn.dfm_smooth,
        rescaling = "none", level = 0.95, verbose = TRUE) 

#extract desired features 
test <- as.data.frame(cbind(ws_prediction@newdata@Dimnames$docs, ws_prediction@textscores$textscore_raw))
names(test) <- c("ID", "Wordscore")

#merge with original data 
amzn <- merge(x=amzn, y=test, by.x="ID", by.y="ID")
amzn$Wordscore <- as.numeric(as.character(amzn$Wordscore))

#create rank by wordscore 
amzn <- amzn[with(amzn, order(amzn$Wordscore, decreasing=TRUE)) ,]
amzn$Word_Rank <- seq(1:10000)

#compute abs difference between two ranks
rank_sum_ws <- sum(abs(amzn$Word_Rank - amzn$Act_Rank))

```


#Problem 2(d) - answer - rank sum 

#####Based on the difference between the absolute values of the differences between actual and predicted scores, the dictionary method performed better than the wordscore method. 
```{r, message=FALSE, warning=FALSE}


#Wordscores ranking
summary(amzn$Word_Rank)

#Absolute difference between wordscores and actual score 
rank_sum_ws

```







#Problem 2(e) - creating SVM model 

#####Pre-processing: Preprocessing involved removing stemwords and punctuation, as the other characters were already removed in a previous step. 



```{r, message=FALSE, warning=FALSE}

library(NLP)
library(tm)
library(RTextTools)
library(wordcloud)


#create function to use for testing different training sizes 
splt_data <- function(x, kerntype) {
  
  #subset data to just first 1,000 entries for purposes of time 
  amzn_svm <- amzn[1:1000, ]
  #create weighted TfIdf matrix with training text 
  amzn_svm.dtm <- create_matrix(amzn_svm$Text, language="english", stemWords = FALSE,
                             weighting = weightTfIdf, removePunctuation = FALSE)
  #create container with document term matrix and appropriate classes for training data 
  container <- create_container(amzn_svm.dtm, t(amzn_svm$Class), trainSize=1:x,
                            testSize=(x+1):nrow(amzn_svm),    virgin=FALSE)
  #perform cross validation with 5 folds and the appropriate kernel type 
  cv.svm <- cross_validate(container, nfold=5, algorithm = 'SVM', kernel = kerntype)
  
  #return average accuracy 
  return (cv.svm$meanAccuracy)
  

}


#try with different ratios of test/training

#10/90
train_size <- seq(1:9)
train_size <- as.data.frame(train_size)
kerntype <- 'linear'
lin_time <- system.time(for (i in 1:nrow(train_size)) {
  split <- train_size$train_size[i]*100
  try <- splt_data(split, kerntype)
  train_size$acc_linear[i] <- try })



kerntype <- 'radial'

rad_time <- system.time(for (i in 1:nrow(train_size)) {
  split <- train_size$train_size[i]*100
  try2 <- splt_data(split, kerntype)
  train_size$acc_radial[i] <- try2 })


train_size$train_size <- train_size$train_size * 100 



```


#Problem 2(e) - answer - creating SVM model 

#####An advantage of SVMs or Naive Bayes relative to the dictionary or wordscores approach is that you can be more specific with your classifications. Whereas the dictionary or wordscores approaches require you to dichotomize everything into "positive" and "negative", you miss picking up on language that might fall out of that spectrum, but is equally important to studty. With SVMs, you can train your model to also identify "middle of the road" examples and then study their use of language. 

#####With respect to kernel selection, here I predicted that the best kernel would be of the linear sort. I made this decision based off the assumption that we have data that is linearly separable, with a high ratio of features to documents. Because of this, it is unnecessary to project the data into a three dimensions and could actually lead to overfitting. Using a radial kernel is also more expensive computationally, especially as the number of documents increases. My prediction was correct, as the linear kernel performed better and faster than the radial. 

#####Here it appears that the optimium training size is around 50-70% of the original data set. When only given 10-20% of the data the model appears to underfit, and thus has poor predicative power because of high variane when applied to test folds. Between70-90%,we see problems at the other end of the spectrum - we have little bias (i.e. a complex model), but high variance, as the model does not generalize well too virgin data. 


```{r, message=FALSE, warning=FALSE}

#Table with averages for linear and radial kernels 
train_size

#training times for reference 
lin_time
rad_time 

```






#Question 3 - differences by nationality 








```{r, message=FALSE, warning=FALSE}





#read in data

hit <- read.csv("CF_rate_trustworthiness.csv")

hit$demo_group <- as.factor(gsub("[0-9]", "", hit$image_name))

hit$X_country <- as.factor(hit$X_country)
#create ANOVA model for checking differences between nationalities 
model <- lm(rating ~ X_country, data=hit)

```




#Question 3 - answer - differences by nationality 

#####Is there any nationality that is likely to give statistically significant higher than average ratings? No. Because an effort was made to collect responses from a wide variety of nationalities, and the dataset is very small (250) compared to the number of countries that exist, it is very unlikely that the difference in average rating by nationality will be statistically significant. As you can see, considering an alpha of .01, this holds true. 



```{r, message=FALSE, warning=FALSE}
model 

summary(model)





```


#Question 3 - differences by demographic 






```{r, message=FALSE, warning=FALSE}




#create ANOVA model for checking differences between groups
model <- lm(rating ~ demo_group, data=hit)
demo_anova <- anova(model)



```

#Question 3 - answer - differences by demographic 

#####Yes, an analysis of variance shows that there are statistically significant differences in the average ratings of each demographic group. 

```{r, message=FALSE, warning=FALSE}

demo_anova
summary(demo_anova)

```


#Question 3  - differences by gender 


```{r, message=FALSE, warning=FALSE}

#create ANOVA model for just women and men 

hit$gender <- gsub("black", "", hit$demo_group)
hit$gender <- gsub("white", "", hit$gender)


model_gen <- lm(rating ~ gender, data=hit)
gen_anova <- anova(model_gen)
```


#Question 3 - answer - differences by gender 

#####There is only a statistical difference if you choose an alpha of > .05, which I would not in this case, because of the sensitive nature of the research. 

```{r, message=FALSE, warning=FALSE}

summary(gen_anova)
gen_anova
```


#Question 4 - answer - questions to CrowdFlower raters 

#####I would probably ask them to specify their race - as it would be helpful to know if there is some effect of "rater bias" as far as the race of the rater is concerned. With that information, we might be able to control for that issue. I would also ask them to specify their trust of politicians as a whole. In the case that the sample of politician race is not equally represented, it would be helpful to know what proportion of trustworthiness corresponds to "race" and what proportion is due to that rater's inherent distrust of politicians in general. 











